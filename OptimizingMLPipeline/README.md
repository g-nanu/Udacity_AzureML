# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data pertaining to direct marketing campaigns of a banking institution.
The classification goal is to predict whether the client will accept a term deposit or not.

In the given project solution, we have implemented the solution here using scikit-learn Logistic Regression and HyperDrive for hyperparameter tuning. We have also used AutoML to find the most optimized model for the given dataset and compared the results of the above two methods. The best performing model was obtained through AutoML - VotingEnsemble with accuracy of 0.914

## Scikit-learn Pipeline
    i) Importing data as tabular dataset using TabularDatasetFactory from url - "https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv".
    ii) Data is then cleaned and pre-processed using clean_data function, this includes Binary encoding and One Hot Encoding of categorical features
    iii) Splitting of data into train and test data in 80:20 ratio
    iv) Defined a Scikit-learn based Logistic Regression model. We define 2 hyperparameters C and max_iter for tuning. C stands for the inverse regularization parameter and max_iter represents the maximum number of iterations.
    v)Using a SKLearn estimator and parameter sampler we created HyperDrive configuration.
    vi)Accuracy is calculated on the test set for each run and the model which had best accuracy was saved.

**What are the benefits of the parameter sampler you chose?**

The parameter sampler I chose was RandomParameterSampling because it supports both discrete and continuous hyperparameters. It supports early termination of low-performance runs and supports early stopping policies. In random sampling , the hyperparameter (C : smaller values specify stronger regularization, max_iter : maximum number of iterations taken for the solvers to converge) values are randomly selected from the defined search space

random sampling on the hyperparameter search was done using RandomParameterSampling as it supported discrete and continuos hyperparameters.it significantly reduces computation time as It supports early termination of low-performance runs and supports early stopping policies. it provided us acceptable results.



**What are the benefits of the early stopping policy you chose?**

BanditPolicy was the one i choose as early stopping policy as it helps us to execute more runs than other policies like the MedianStoppingPolicy, which results in lesser computational time. it is based on slack factor and evaluation interval. Bandit terminates runs where the primary metric is not within the specified slack factor compared to the best performing run.


## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML provided the ability to run multiple experiments and choose best clasfication model using the provided dataset. 
AutoML ran 10+ different classification models out of which in our case VotingEnsemble algorithm proved to be the best model with an accuracy of 91.4%.

Voting Ensemble

the VotingClassifier combines conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses.

we can use AutoML console to see in depth model explanation, eg. picture below:






## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
